diff a/core/TD3.py b/core/TD3.py	(rejected hunks)
@@ -161,8 +161,10 @@ class TD3(OffPolicyAlgorithm):
             ideal_data = False
             buffers = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)
             expert_replay_data = self.expert_replay_buffer.sample(batch_size, env=self._vec_normalize_env)
+            constrained_replay_data = self.constrained_replay_buffer.sample(batch_size, env=self._vec_normalize_env)
 
-            nsteps = expert_replay_data.nstep[0].numpy()[0]
+            nsteps = 10#expert_replay_data.nstep[0].numpy()[0]
+            #print(self.sl_dataset.enums)
             if len(buffers) > 1:
                 replay_data, ideal_replay_data, combined_replay_data = buffers[0], buffers[1], buffers[2]
                 ideal_data = True
@@ -187,29 +189,66 @@ class TD3(OffPolicyAlgorithm):
                 # Compute the LfD n-step bootstrap Q value and the action after n-steps
                 noise = expert_replay_data.nth_actions.clone().data.normal_(0, self.target_policy_noise)
                 noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)
-                next_actions = (self.actor_target(expert_replay_data.nth_observations) + noise).clamp(-1, 1)
+                nth_actions = (self.actor_target(expert_replay_data.nth_observations) + noise).clamp(-1, 1)
 
                 targets_expert = th.cat(self.critic_target(expert_replay_data.nth_observations.float(), 
-                                                                next_actions.float()), dim=1)
+                                                                nth_actions.float()), dim=1)
                 target_q_expert, _ = th.min(targets_expert, dim=1, keepdim=True)
-                target_q_expert = expert_replay_data.rewards + expert_replay_data.optimal_nstep_R + (1 - expert_replay_data.dones) * self.gamma**nsteps * target_q_expert
+                target_q_expert = expert_replay_data.rewards + expert_replay_data.optimal_nstep_R + (1 - expert_replay_data.dones) * expert_replay_data.nstep_gamma * target_q_expert
                 
-                # Compute the Lower bound and upper bound of Q when <s,a> is similar to exoe
-
+                # Compute the Lower bound and upper bound of Q when <s,a> is similar to expert
+                #"""
+                noise = constrained_replay_data.nth_actions.clone().data.normal_(0, self.target_policy_noise)
+                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)
+                nth_actions = (self.actor_target(constrained_replay_data.nth_observations) + noise).clamp(-1, 1)
+                targets_constrained = th.cat(self.critic_target(constrained_replay_data.nth_observations.float(), 
+                                                                nth_actions.float()), dim=1)
+                target_q_constrained_, _ = th.min(targets_constrained, dim=1, keepdim=True)
+                target_q_real_nstep = constrained_replay_data.optimal_nstep_R + (1 - constrained_replay_data.dones) * constrained_replay_data.nstep_gamma * target_q_constrained_
+                target_q_sub_nstep = constrained_replay_data.subopt_values + (1 - constrained_replay_data.dones) * constrained_replay_data.nstep_gamma * target_q_constrained_
+                
+                noise = constrained_replay_data.actions.clone().data.normal_(0, self.target_policy_noise)
+                noise = noise.clamp(-self.target_noise_clip, self.target_noise_clip)
+                next_actions = (self.actor_target(constrained_replay_data.next_observations) + noise).clamp(-1, 1)
+                targets_constrained = th.cat(self.critic_target(constrained_replay_data.next_observations.float(), 
+                                                                next_actions.float()), dim=1)
+                target_q_constrained_, _ = th.min(targets_constrained, dim=1, keepdim=True)
+                target_q_constrained = constrained_replay_data.rewards + (1 - constrained_replay_data.dones) * self.gamma * target_q_constrained_
+                
+                target_q_lower_bound, _ = th.max(th.cat((target_q_real_nstep, target_q_constrained), dim=1),dim=1, keepdim=True)
+                #target_q_lower_bound, _ = th.max(th.cat((target_q_lower_bound, target_q_constrained), dim=1),dim=1, keepdim=True)
+                #print(target_q_lower_bound)
+                #"""
             # Get current Q estimates for each critic network
             current_q_estimates = self.critic(replay_data.observations.float(), replay_data.actions.float())
             # Compute critic loss
             critic_loss_origin = sum([F.mse_loss(current_q, target_q) for current_q in current_q_estimates])
 
-            constrained_current_q_estimates  = self.critic(expert_replay_data.observations.float(), expert_replay_data.actions.float())
+            expert_current_q_estimates  = self.critic(expert_replay_data.observations.float(), expert_replay_data.actions.float())
+            critic_loss_expert = sum([F.mse_loss(current_q, target_q_expert) for current_q in expert_current_q_estimates])
+
+            #"""
+            constrained_current_q_estimates = self.critic(constrained_replay_data.observations.float(), constrained_replay_data.actions.float())
+            constrained_current_q_estimates1, constrained_current_q_estimates2  =  constrained_current_q_estimates
+            zero_tensors = th.zeros(constrained_current_q_estimates1.shape)
+            lower_bound_filtered_q_dis1, _ = th.max(th.cat(((constrained_current_q_estimates1 - target_q_real_nstep), zero_tensors), dim=1),dim=1, keepdim=True)
+            lower_bound_filtered_q_dis2, _ = th.max(th.cat(((constrained_current_q_estimates2 - target_q_real_nstep), zero_tensors), dim=1),dim=1, keepdim=True)
+            #print(lower_bound_filtered_q_dis1)
+            critic_loss_constrained1 = th.mean(lower_bound_filtered_q_dis1**2)
+            critic_loss_constrained2 = th.mean(lower_bound_filtered_q_dis2**2)
+            critic_loss_constrained = sum([critic_loss_constrained1, critic_loss_constrained2])
+            #print(critic_loss_constrained)
+            #critic_loss_constrained = sum([F.mse_loss(current_q, target_q_real_nstep) for current_q in constrained_current_q_estimates])
+            #"""
+
+
             #critic_loss_expert1 = F.mse_loss(constrained_current_q_estimates[0]*loss_mask, target_q2*loss_mask)
             #critic_loss_expert2 = F.mse_loss(constrained_current_q_estimates[1]*loss_mask, target_q2*loss_mask)
             #critic_loss_expert = sum([critic_loss_expert1, critic_loss_expert2])
-            critic_loss_expert = sum([F.mse_loss(current_q, target_q_expert) for current_q in constrained_current_q_estimates])
-            #if critic_loss_expert > 0:
-            #    print(critic_loss_expert)
+            #if critic_loss_constrained > 0:
+            #    print(critic_loss_constrained)
            
-            critic_loss = critic_loss_origin + critic_loss_expert 
+            critic_loss = critic_loss_origin + 0.*critic_loss_expert + 0.*critic_loss_constrained
             critic_losses.append(critic_loss.item())
 
             # Optimize the critics
@@ -359,6 +398,63 @@ class TD3(OffPolicyAlgorithm):
 
         polyak_update(self.actor.parameters(), self.actor_target.parameters(), 1)
 
+
+    def add_expert_trajs_to_buffer(self, parsed_trajs, value_dataset):
+        for traj in parsed_trajs:
+            traj = np.array(traj)
+            for i in range(len(traj) - 10):
+                prev_obs = traj[i][0]
+                act = traj[i][1]
+                obs = traj[i][2]
+                r = traj[i][3]
+
+                discounted_sub_R = 0
+                for idx, sub_trans in enumerate(traj[i:i + 10]):
+                    discounted_sub_R += value_dataset.reward_gamma**(idx) * sub_trans[3]
+
+                if value_dataset.value_type == 'v':
+                    inputs = th.FloatTensor(np.array([prev_obs]))
+                    sub_Q = value_dataset.sub_q_model.model(inputs).detach().numpy().flatten()[0]
+                    opt_Q = value_dataset.opt_q_model.model(inputs).detach().numpy().flatten()[0]
+                elif value_dataset.value_type == 'q':
+                    inputs = th.FloatTensor(np.array([np.hstack([prev_obs, act])]))
+                    sub_Q = value_dataset.sub_q_model.model(inputs).detach().numpy().flatten()[0]
+                    opt_Q = value_dataset.opt_q_model.model(inputs).detach().numpy().flatten()[0]
+
+                self.expert_replay_buffer.add(prev_obs,
+                                               obs,
+                                               act,
+                                               r,
+                                               False,
+                                               sub_Q,
+                                               opt_Q,
+                                               discounted_sub_R,
+                                               discounted_sub_R,
+                                               traj[i + 10][0],
+                                               traj[i + 10][1],
+                                               value_dataset.reward_gamma**10)
+
+                self.replay_buffer.add(prev_obs,
+                                       obs,
+                                       act,
+                                       r,
+                                       False, None, None, None, None, None, use_ideal=False)
+
+                self.constrained_replay_buffer.add(prev_obs,
+                                               obs,
+                                               act,
+                                               r,
+                                               False,
+                                               sub_Q,
+                                               opt_Q,
+                                               discounted_sub_R,
+                                               discounted_sub_R,
+                                               traj[i + 10][0],
+                                               traj[i + 10][1],
+                                               value_dataset.reward_gamma**10)
+
+                print(discounted_sub_R, r, i)
+
     def _excluded_save_params(self) -> List[str]:
         return super(TD3, self)._excluded_save_params() + ["actor", "critic", "actor_target", "critic_target"]
 
